# -*- coding: utf-8 -*-
"""Face_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jdFqlm_VPvZECTHPI-S6r1L1ALiawVlx
"""

# Import necessary libraries
import os
from PIL import Image
import numpy as np
import cv2
import face_recognition
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from imutils.video import VideoStream
from tqdm import tqdm
from collections import defaultdict

# Function to resize images (alternative to deprecated imresize)
def imresize(arr, size):
    return np.array(Image.fromarray(arr).resize(size, Image.BILINEAR))

# Image size setting
IMG_SIZE = 24

# Data generator for image augmentation and normalization
def collect():
    try:
        train_datagen = ImageDataGenerator(
            rescale=1./255,
            shear_range=0.2,
            horizontal_flip=True
        )

        val_datagen = ImageDataGenerator(
            rescale=1./255,
            shear_range=0.2,
            horizontal_flip=True
        )

        train_generator = train_datagen.flow_from_directory(
            directory="dataset/dataset/train",
            target_size=(IMG_SIZE, IMG_SIZE),
            color_mode="grayscale",
            batch_size=32,
            class_mode="binary",
            shuffle=True,
            seed=42
        )

        val_generator = val_datagen.flow_from_directory(
            directory="dataset/dataset/val",
            target_size=(IMG_SIZE, IMG_SIZE),
            color_mode="grayscale",
            batch_size=32,
            class_mode="binary",
            shuffle=True,
            seed=42
        )
        return train_generator, val_generator

    except Exception as e:
        print(f"[ERROR] Failed to collect data: {e}")
        return None, None

# Function to save the trained model
def save_model(model):
    try:
        model.save('eye_status_classifier.h5')
    except Exception as e:
        print(f"[ERROR] Failed to save model: {e}")

# Function to load a pre-trained model
def load_pretrained_model():
    try:
        model = load_model('eye_status_classifier.h5')
        model.summary()
        return model
    except Exception as e:
        print(f"[ERROR] Failed to load model: {e}")
        return None

# Function to train the model
def train(train_generator, val_generator):
    STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size
    STEP_SIZE_VALID = val_generator.n // val_generator.batch_size

    print('[LOG] Initialize Neural Network')

    model = Sequential()
    model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))
    model.add(MaxPooling2D())
    model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D())
    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D())
    model.add(Flatten())
    model.add(Dense(units=120, activation='relu'))
    model.add(Dense(units=84, activation='relu'))
    model.add(Dense(units=1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    try:
        model.fit(train_generator,
                  steps_per_epoch=STEP_SIZE_TRAIN,
                  validation_data=val_generator,
                  validation_steps=STEP_SIZE_VALID,
                  epochs=20
        )
        save_model(model)
    except Exception as e:
        print(f"[ERROR] Model training failed: {e}")

# Function to predict eye status
def predict(img, model):
    img = Image.fromarray(img)  # Convert the numpy array to a PIL Image
    img = img.convert('L')  # Convert to grayscale
    img = img.resize((IMG_SIZE, IMG_SIZE))  # Resize using PIL
    img = np.array(img).astype('float32')  # Convert back to numpy array
    img /= 255  # Normalize
    img = img.reshape(1, IMG_SIZE, IMG_SIZE, 1)  # Reshape for model
    prediction = model.predict(img)
    if prediction < 0.1:
        prediction = 'closed'
    elif prediction > 0.9:
        prediction = 'open'
    else:
        prediction = 'idk'
    return prediction

# Function to evaluate the model
def evaluate(X_test, y_test):
    model = load_pretrained_model()
    if model is None:
        print("[ERROR] No model loaded. Cannot evaluate.")
        return
    print('Evaluate model')
    try:
        loss, acc = model.evaluate(X_test, y_test, verbose=0)
        print(f"Accuracy: {acc * 100:.2f}%")
    except Exception as e:
        print(f"[ERROR] Model evaluation failed: {e}")

# Function to initialize the face and eye detectors
def init():
    try:
        face_cascPath = 'haarcascade_frontalface_alt.xml'
        open_eye_cascPath = 'haarcascade_eye_tree_eyeglasses.xml'
        left_eye_cascPath = 'haarcascade_lefteye_2splits.xml'
        right_eye_cascPath = 'haarcascade_righteye_2splits.xml'
        dataset = 'faces'

        face_detector = cv2.CascadeClassifier(face_cascPath)
        open_eyes_detector = cv2.CascadeClassifier(open_eye_cascPath)
        left_eye_detector = cv2.CascadeClassifier(left_eye_cascPath)
        right_eye_detector = cv2.CascadeClassifier(right_eye_cascPath)

        model = load_pretrained_model()

        if model is None:
            raise Exception("Model could not be loaded.")

        print("[LOG] Collecting images ...")
        images = []
        for direc, _, files in tqdm(os.walk(dataset)):
            for file in files:
                if file.endswith("jpg"):
                    images.append(os.path.join(direc, file))
        return model, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, images

    except Exception as e:
        print(f"[ERROR] Initialization failed: {e}")
        return None, None, None, None, None, []

# Function to process and encode face images
def process_and_encode(image_paths):
    known_encodings = []
    known_names = []
    print("[LOG] Encoding faces ...")

    if not image_paths:
        print("[ERROR] No images found to process.")
        return {"encodings": [], "names": []}

    for image_path in tqdm(image_paths):
        if not os.path.isfile(image_path):
            print(f"[WARNING] Image path {image_path} does not exist.")
            continue

        image = cv2.imread(image_path)
        if image is None:
            print(f"[WARNING] Could not read image {image_path}.")
            continue

        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        boxes = face_recognition.face_locations(image, model='cnn')
        encodings = face_recognition.face_encodings(image, boxes)
        name = os.path.basename(os.path.dirname(image_path))

        if encodings:
            known_encodings.append(encodings[0])
            known_names.append(name)
        else:
            print(f"[WARNING] No face encodings found in {image_path}.")

    if not known_encodings:
        print("[ERROR] No face encodings were found.")
        return {"encodings": [], "names": []}

    encodings = {"encodings": known_encodings, "names": known_names}
    np.save('encodings.npy', encodings)

    return encodings

# Function to check if eyes are blinking
def isBlinking(history, maxFrames):
    for i in range(maxFrames):
        pattern = '1' + '0' * (i+1) + '1'
        if pattern in history:
            return True
    return False

# Function to detect and display face and eye status
def detect_and_display(model, video_capture, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, data, eyes_detected):
    frame = video_capture.read()
    if frame is None:
        print("[ERROR] Failed to capture frame.")
        return

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    faces = face_detector.detectMultiScale(
        gray,
        scaleFactor=1.2,
        minNeighbors=5,
        minSize=(50, 50),
        flags=cv2.CASCADE_SCALE_IMAGE
    )

    for (x, y, w, h) in faces:
        encoding = face_recognition.face_encodings(rgb, [(y, x+w, y+h, x)])[0]
        matches = face_recognition.compare_faces(data["encodings"], encoding)
        name = "Unknown"

        if True in matches:
            matchedIdxs = [i for (i, b) in enumerate(matches) if b]
            counts = {}
            for i in matchedIdxs:
                name = data["names"][i]
                counts[name] = counts.get(name, 0) + 1
            name = max(counts, key=counts.get)

        face = frame[y:y+h, x:x+w]
        gray_face = gray[y:y+h, x:x+w]

        open_eyes_glasses = open_eyes_detector.detectMultiScale(
            gray_face,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(30, 30),
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        if len(open_eyes_glasses) == 2:
            eyes_detected[name] += '1'
            for (ex, ey, ew, eh) in open_eyes_glasses:
                cv2.rectangle(face, (ex, ey), (ex+ew, ey+eh), (0, 255, 0), 2)
        else:
            left_face = frame[y:y+h, x+int(w/2):x+w]
            left_face_gray = gray[y:y+h, x+int(w/2):x+w]

            right_face = frame[y:y+h, x:x+int(w/2)]
            right_face_gray = gray[y:y+h, x+int(w/2)]

            left_eye = left_eye_detector.detectMultiScale(
                left_face_gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30),
                flags=cv2.CASCADE_SCALE_IMAGE
            )

            right_eye = right_eye_detector.detectMultiScale(
                right_face_gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30),
                flags=cv2.CASCADE_SCALE_IMAGE
            )

            eye_status = '1'

            for (ex, ey, ew, eh) in right_eye:
                color = (0, 255, 0)
                pred = predict(right_face[ey:ey+eh, ex:ex+ew], model)
                if pred == 'closed':
                    color = (0, 0, 255)
                    eye_status = '0'
                cv2.rectangle(right_face, (ex, ey), (ex+ew, ey+eh), color, 2)
                break

            for (ex, ey, ew, eh) in left_eye:
                color = (0, 255, 0)
                pred = predict(left_face[ey:ey+eh, ex:ex+ew], model)
                if pred == 'closed':
                    color = (0, 0, 255)
                    eye_status = '0'
                cv2.rectangle(left_face, (ex, ey), (ex+ew, ey+eh), color, 2)
                break

            eyes_detected[name] += eye_status

        if isBlinking(eyes_detected[name], 3):
            print('[INFO] {} is blinking'.format(name))
            cv2.putText(frame, 'Blink detected', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            # Save the image
            cv2.imwrite(f'blink_detected_{name}.jpg', frame)
        else:
            cv2.putText(frame, 'No blink', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        cv2.putText(frame, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

    cv2.imshow("Frame", frame)

# Main loop to run the face and blink detection
def main():
    train_generator, val_generator = collect()
    if train_generator is None or val_generator is None:
        print("[ERROR] Data collection failed. Exiting.")
        return

    train(train_generator, val_generator)

    model, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, images = init()
    if model is None:
        print("[ERROR] Initialization failed. Exiting.")
        return

    data = process_and_encode(images)
    eyes_detected = defaultdict(str)
    video_capture = VideoStream(src=0).start()

    while True:
        detect_and_display(model, video_capture, face_detector, open_eyes_detector, left_eye_detector, right_eye_detector, data, eyes_detected)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    video_capture.stop()
    cv2.destroyAllWindows()

if __name__ == '__main__':
    main()
